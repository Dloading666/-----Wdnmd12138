"""
è™æ‰‘ç½‘ç«™æ–°é—»é‡‡é›†å·¥å…·
"""
import requests
from bs4 import BeautifulSoup
from typing import List, Dict, Optional
from datetime import datetime
import re
import time

class HupuScraper:
    """è™æ‰‘æ–°é—»é‡‡é›†å™¨ - æ”¯æŒAPIæ¥å£å’Œç½‘é¡µçˆ¬å–"""
    
    def __init__(self):
        self.base_url = "https://www.hupu.com"
        self.api_base_url = "https://bbs.hupu.com/v1"
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'application/json, text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
            'Referer': 'https://www.hupu.com/',
            'Origin': 'https://www.hupu.com'
        }
    
    def get_news_from_api(self, category: str = "nba", page: int = 1, limit: int = 20) -> Optional[List[Dict]]:
        """
        é€šè¿‡è™æ‰‘APIæ¥å£è·å–æ–°é—»ï¼ˆä¼˜å…ˆä½¿ç”¨ï¼‰
        
        Args:
            category: æ–°é—»ç±»åˆ« (nba, soccer, cbaç­‰)
            page: é¡µç 
            limit: æ¯é¡µæ•°é‡
        
        Returns:
            æ–°é—»åˆ—è¡¨ï¼Œå¦‚æœAPIä¸å¯ç”¨è¿”å›None
        """
        try:
            # è™æ‰‘APIæ¥å£
            api_url = f"{self.api_base_url}/news/{category}"
            params = {
                'page': page,
                'limit': limit
            }
            
            print(f"ğŸ“¡ å°è¯•ä½¿ç”¨è™æ‰‘APIè·å–æ•°æ®: {api_url}")
            
            response = requests.get(api_url, headers=self.headers, params=params, timeout=15)
            
            if response.status_code == 200:
                try:
                    data = response.json()
                    print(f"âœ“ è™æ‰‘APIè¿”å›æ•°æ®: {type(data)}")
                    
                    # è§£æAPIè¿”å›çš„æ•°æ®ç»“æ„
                    news_list = []
                    
                    # å°è¯•ä¸åŒçš„æ•°æ®ç»“æ„
                    if isinstance(data, dict):
                        # å¯èƒ½çš„ç»“æ„: {"data": [...], "list": [...], "news": [...]}
                        items = data.get('data') or data.get('list') or data.get('news') or data.get('result', [])
                    elif isinstance(data, list):
                        items = data
                    else:
                        print(f"âš ï¸ æœªçŸ¥çš„APIæ•°æ®ç»“æ„: {type(data)}")
                        return None
                    
                    if not items:
                        print(f"âš ï¸ APIè¿”å›æ•°æ®ä¸ºç©º")
                        return None
                    
                    print(f"âœ“ ä»APIè·å–åˆ° {len(items)} æ¡æ•°æ®")
                    
                    # è§£ææ¯æ¡æ–°é—»
                    for item in items[:limit]:
                        try:
                            # å¤„ç†ä¸åŒçš„æ•°æ®ç»“æ„
                            if isinstance(item, dict):
                                news = {
                                    "title": item.get('title') or item.get('headline') or item.get('name', ''),
                                    "content": item.get('content') or item.get('summary') or item.get('description') or item.get('title', ''),
                                    "source": item.get('source') or item.get('author') or "è™æ‰‘",
                                    "url": item.get('url') or item.get('link') or item.get('href', ''),
                                    "category": self._map_category(category),
                                    "publish_time": self._parse_api_time(item.get('time') or item.get('publish_time') or item.get('date')),
                                    "metadata": {
                                        "source_site": "è™æ‰‘",
                                        "source_type": "API",
                                        "category_code": category,
                                        "api_data": item  # ä¿ç•™åŸå§‹APIæ•°æ®
                                    }
                                }
                                
                                # æ™ºèƒ½è¯†åˆ«ç±»åˆ«
                                detected_category = self._detect_category_from_content(news['title'], news['content'])
                                if detected_category != 'ä½“è‚²':
                                    news['category'] = detected_category
                                
                                if news['title']:
                                    news_list.append(news)
                        except Exception as e:
                            print(f"âš ï¸ è§£æAPIæ–°é—»é¡¹å¤±è´¥: {str(e)}")
                            continue
                    
                    if news_list:
                        print(f"âœ“ æˆåŠŸä»è™æ‰‘APIè·å– {len(news_list)} æ¡æ–°é—»")
                        return news_list
                    else:
                        print(f"âš ï¸ APIæ•°æ®è§£æåä¸ºç©º")
                        return None
                        
                except ValueError as e:
                    # JSONè§£æé”™è¯¯
                    print(f"âš ï¸ APIè¿”å›éJSONæ•°æ®: {str(e)}")
                    print(f"   å“åº”å†…å®¹å‰100å­—ç¬¦: {response.text[:100]}")
                    return None
            else:
                print(f"âš ï¸ è™æ‰‘APIè¯·æ±‚å¤±è´¥ï¼ŒçŠ¶æ€ç : {response.status_code}")
                return None
                
        except requests.exceptions.Timeout:
            print(f"âš ï¸ è™æ‰‘APIè¯·æ±‚è¶…æ—¶")
            return None
        except requests.exceptions.ConnectionError as e:
            print(f"âš ï¸ è™æ‰‘APIè¿æ¥å¤±è´¥: {str(e)}")
            return None
        except Exception as e:
            print(f"âš ï¸ è™æ‰‘APIè°ƒç”¨å¤±è´¥: {str(e)}")
            import traceback
            traceback.print_exc()
            return None
    
    def _parse_api_time(self, time_value) -> datetime:
        """è§£æAPIè¿”å›çš„æ—¶é—´"""
        if not time_value:
            return datetime.now()
        
        try:
            # å¦‚æœæ˜¯æ—¶é—´æˆ³
            if isinstance(time_value, (int, float)):
                return datetime.fromtimestamp(time_value)
            
            # å¦‚æœæ˜¯å­—ç¬¦ä¸²
            if isinstance(time_value, str):
                # å°è¯•å¤šç§æ—¶é—´æ ¼å¼
                time_formats = [
                    '%Y-%m-%d %H:%M:%S',
                    '%Y-%m-%dT%H:%M:%S',
                    '%Y-%m-%dT%H:%M:%SZ',
                    '%Y-%m-%d %H:%M',
                    '%Y/%m/%d %H:%M:%S',
                    '%Y/%m/%d %H:%M',
                ]
                
                for fmt in time_formats:
                    try:
                        return datetime.strptime(time_value, fmt)
                    except:
                        continue
                
                # å¦‚æœæ˜¯æ—¶é—´æˆ³å­—ç¬¦ä¸²
                try:
                    return datetime.fromtimestamp(float(time_value))
                except:
                    pass
        except:
            pass
        
        return datetime.now()
    
    def get_news_list(self, category: str = "nba", limit: int = 10, use_api: bool = True) -> List[Dict]:
        """
        è·å–è™æ‰‘æ–°é—»åˆ—è¡¨ï¼ˆä¼˜å…ˆä½¿ç”¨APIï¼Œå¤±è´¥åˆ™ä½¿ç”¨ç½‘é¡µçˆ¬å–ï¼‰
        
        Args:
            category: æ–°é—»ç±»åˆ« (nba, soccer, cba, etc.)
            limit: è·å–æ•°é‡é™åˆ¶
            use_api: æ˜¯å¦ä¼˜å…ˆä½¿ç”¨APIæ¥å£
        
        Returns:
            æ–°é—»åˆ—è¡¨
        """
        # 1. ä¼˜å…ˆå°è¯•ä½¿ç”¨APIæ¥å£
        if use_api:
            api_news = self.get_news_from_api(category, page=1, limit=limit)
            if api_news and len(api_news) > 0:
                return api_news[:limit]
            else:
                print("âš ï¸ è™æ‰‘APIä¸å¯ç”¨ï¼Œé™çº§åˆ°ç½‘é¡µçˆ¬å–")
        
        # 2. å¤‡ç”¨æ–¹æ¡ˆï¼šç½‘é¡µçˆ¬å–
        try:
            # è™æ‰‘æ–°é—»åˆ—è¡¨é¡µURL
            url = f"{self.base_url}/{category}"
            
            response = requests.get(url, headers=self.headers, timeout=10)
            response.encoding = 'utf-8'
            
            if response.status_code != 200:
                # å¦‚æœä¸»ç«™ä¸å¯ç”¨ï¼Œå°è¯•ç§»åŠ¨ç«¯
                url = f"https://m.hupu.com/{category}"
                response = requests.get(url, headers=self.headers, timeout=10)
                response.encoding = 'utf-8'
            
            soup = BeautifulSoup(response.text, 'html.parser')
            
            news_list = []
            
            # å°è¯•å¤šç§é€‰æ‹©å™¨æ¥åŒ¹é…è™æ‰‘çš„æ–°é—»åˆ—è¡¨ç»“æ„
            selectors = [
                'div.news-list-item',
                'div.list-item',
                'a.news-item',
                'div.news-item',
                'li.news-item',
                'div[class*="news"]',
                'a[href*="/news/"]',
                'a[href*="/article/"]'
            ]
            
            items = []
            for selector in selectors:
                items = soup.select(selector)
                if items:
                    break
            
            # å¦‚æœæ‰¾ä¸åˆ°æ ‡å‡†ç»“æ„ï¼Œå°è¯•ä»é“¾æ¥ä¸­æå–
            if not items:
                items = soup.find_all('a', href=re.compile(r'/(news|article|bbs)/'))
            
            for item in items[:limit]:
                try:
                    news = self._parse_news_item(item, category)
                    if news and news.get('title'):
                        news_list.append(news)
                except Exception as e:
                    print(f"è§£ææ–°é—»é¡¹å¤±è´¥: {str(e)}")
                    continue
            
            return news_list[:limit]
            
        except Exception as e:
            print(f"è·å–è™æ‰‘æ–°é—»åˆ—è¡¨å¤±è´¥: {str(e)}")
            return []
    
    def _parse_news_item(self, item, category: str) -> Optional[Dict]:
        """è§£æå•ä¸ªæ–°é—»é¡¹"""
        try:
            # æå–æ ‡é¢˜
            title_elem = item.find(['h3', 'h2', 'h1', 'a', 'span'], class_=re.compile(r'title|headline'))
            if not title_elem:
                title_elem = item.find('a')
            
            title = title_elem.get_text(strip=True) if title_elem else ""
            
            # æå–é“¾æ¥
            link_elem = item.find('a') if item.name != 'a' else item
            url = link_elem.get('href', '') if link_elem else ''
            if url and not url.startswith('http'):
                url = self.base_url + url
            
            # æå–æ‘˜è¦/å†…å®¹
            content_elem = item.find(['p', 'div', 'span'], class_=re.compile(r'content|summary|desc|intro'))
            content = content_elem.get_text(strip=True) if content_elem else ""
            
            # æå–æ—¶é—´
            time_elem = item.find(['span', 'div', 'time'], class_=re.compile(r'time|date|publish'))
            publish_time = None
            if time_elem:
                time_text = time_elem.get_text(strip=True)
                publish_time = self._parse_time(time_text)
            
            # æå–æ¥æº
            source_elem = item.find(['span', 'div'], class_=re.compile(r'source|author|from'))
            source = source_elem.get_text(strip=True) if source_elem else "è™æ‰‘"
            
            if not title:
                return None
            
            # æ™ºèƒ½è¯†åˆ«ç±»åˆ«ï¼ˆåŸºäºå†…å®¹è€ŒéURLè·¯å¾„ï¼‰
            detected_category = self._detect_category_from_content(title, content or title)
            # å¦‚æœæ£€æµ‹åˆ°çš„ç±»åˆ«ä¸URLç±»åˆ«ä¸ä¸€è‡´ï¼Œä½¿ç”¨æ£€æµ‹åˆ°çš„ç±»åˆ«
            mapped_category = self._map_category(category)
            # ä¼˜å…ˆä½¿ç”¨æ™ºèƒ½æ£€æµ‹çš„ç±»åˆ«ï¼Œå¦‚æœæ£€æµ‹ä¸åˆ°æ‰ä½¿ç”¨URLæ˜ å°„çš„ç±»åˆ«
            final_category = detected_category if detected_category != 'ä½“è‚²' else mapped_category
            
            return {
                "title": title,
                "content": content or title,  # å¦‚æœæ²¡æœ‰å†…å®¹ï¼Œä½¿ç”¨æ ‡é¢˜
                "source": source,
                "url": url,
                "category": final_category,
                "publish_time": publish_time or datetime.now(),
                "metadata": {
                    "source_site": "è™æ‰‘",
                    "category_code": category,
                    "detected_category": detected_category,
                    "original_category": mapped_category
                }
            }
        except Exception as e:
            print(f"è§£ææ–°é—»é¡¹å‡ºé”™: {str(e)}")
            return None
    
    def _parse_time(self, time_text: str) -> Optional[datetime]:
        """è§£ææ—¶é—´æ–‡æœ¬"""
        try:
            # å¤„ç†ç›¸å¯¹æ—¶é—´ï¼ˆå¦‚"2å°æ—¶å‰"ï¼‰
            if 'å°æ—¶å‰' in time_text or 'åˆ†é’Ÿå‰' in time_text or 'å¤©å‰' in time_text:
                return datetime.now()
            
            # å¤„ç†ç»å¯¹æ—¶é—´
            time_formats = [
                '%Y-%m-%d %H:%M:%S',
                '%Y-%m-%d %H:%M',
                '%Y/%m/%d %H:%M:%S',
                '%Y/%m/%d %H:%M',
                '%m-%d %H:%M',
                '%m/%d %H:%M'
            ]
            
            for fmt in time_formats:
                try:
                    return datetime.strptime(time_text, fmt)
                except:
                    continue
            
            return datetime.now()
        except:
            return datetime.now()
    
    def _map_category(self, category_code: str) -> str:
        """æ˜ å°„ç±»åˆ«ä»£ç åˆ°ä¸­æ–‡åç§°"""
        category_map = {
            'nba': 'NBA',
            'soccer': 'è¶³çƒ',
            'cba': 'CBA',
            'bbs': 'ç¤¾åŒº',
            'news': 'ç»¼åˆ',
            'basketball': 'ç¯®çƒ',
            'football': 'è¶³çƒ',
            'lol': 'ç”µç«',
            'esports': 'ç”µç«',
            'kog': 'ç”µç«'
        }
        return category_map.get(category_code, 'ä½“è‚²')
    
    def _detect_category_from_content(self, title: str, content: str) -> str:
        """æ ¹æ®æ ‡é¢˜å’Œå†…å®¹æ™ºèƒ½è¯†åˆ«æ–°é—»ç±»åˆ«"""
        # åˆå¹¶æ ‡é¢˜å’Œå†…å®¹ï¼Œæ ‡é¢˜æƒé‡æ›´é«˜
        text = (title + " " + title + " " + content).lower()  # æ ‡é¢˜é‡å¤ä¸€æ¬¡å¢åŠ æƒé‡
        
        # ç”µç«å…³é”®è¯ï¼ˆé«˜ä¼˜å…ˆçº§ï¼Œé¿å…è¯¯åˆ¤ï¼‰
        esports_keywords = [
            'lol', 'è‹±é›„è”ç›Ÿ', 'ç‹è€…è£è€€', 'kpl', 'lpl', 'dota', 'csgo', 'pubg', 
            'å’Œå¹³ç²¾è‹±', 'ç©¿è¶Šç«çº¿', 'cf', 'valorant', 'æ— ç•å¥‘çº¦', 'apex', 
            'gala', 'tes', 'jdg', 'rng', 'edg', 'fpx', 'ig', 'we', 'omg', 'blg',
            'ç”µç«', 'èŒä¸šè”èµ›', 'moba', 'fps', 'rts', 'mobaæ¸¸æˆ', 'å¥³æª', 'bo3',
            'æµè¨€æ¿', 'ä¸€å›¾æµ', 'jrs', 'ç¥è¯„', 'wcba', 'wcbaä»Šæ—¥', 'wcbaå¸¸è§„èµ›'
        ]
        
        # è¶³çƒå…³é”®è¯
        soccer_keywords = [
            'è¶³çƒ', 'è‹±è¶…', 'è¥¿ç”²', 'æ„ç”²', 'å¾·ç”²', 'æ³•ç”²', 'ä¸­è¶…', 'ä¸–ç•Œæ¯', 
            'æ¬§æ´²æ¯', 'æ¬§å† ', 'äºšå† ', 'å›½è¶³', 'ç”·è¶³', 'å¥³è¶³', 'æ¢…è¥¿', 'cç½—', 
            'å†…é©¬å°”', 'å§†å·´ä½©', 'å“ˆå…°å¾·', 'çš‡é©¬', 'å·´è¨', 'æ›¼è”', 'åˆ©ç‰©æµ¦', 
            'åˆ‡å°”è¥¿', 'æ›¼åŸ', 'é˜¿æ£®çº³', 'æ‹œä»', 'å¤šç‰¹', 'å°¤æ–‡', 'acç±³å…°', 
            'å›½é™…ç±³å…°', 'å·´é»', 'å¤§å·´é»', 'fifa', 'u23', 'u20', 'u17', 'äºšæ´²æ¯',
            'ä¸–é¢„èµ›', 'é¢„é€‰èµ›', 'é—¨å°†', 'è¿›çƒ', 'åŠ©æ”»', 'ç‚¹çƒ', 'ä»»æ„çƒ'
        ]
        
        # NBAå…³é”®è¯
        nba_keywords = [
            'nba', 'æ¹–äºº', 'å‹‡å£«', 'å‡¯å°”ç‰¹äºº', 'çƒ­ç«', 'ç¯®ç½‘', '76äºº', 
            'é›„é¹¿', 'å¤ªé˜³', 'ç‹¬è¡Œä¾ ', 'å¿«èˆ¹', 'æ˜é‡‘', 'ç°ç†Š', 'çˆµå£«', 
            'è©¹å§†æ–¯', 'åº“é‡Œ', 'æœå…°ç‰¹', 'å­—æ¯å“¥', 'ä¸œå¥‘å¥‡', 'çº¦åŸºå¥‡', 
            'æ©æ¯”å¾·', 'å¡”å›¾å§†', 'å¸ƒå…‹', 'è«å…°ç‰¹', 'å­£åèµ›', 'å¸¸è§„èµ›', 
            'æ€»å†³èµ›', 'mvp', 'å¾—åˆ†ç‹', 'ç¯®æ¿ç‹', 'åŠ©æ”»ç‹', 'ä¸‰åˆ†', 'æ‰£ç¯®',
            'nbaå¸¸è§„èµ›', 'nbaå­£åèµ›', 'nbaæ€»å†³èµ›'
        ]
        
        # CBAå…³é”®è¯
        cba_keywords = [
            'cba', 'cbaè”èµ›', 'ä¸­å›½ç”·ç¯®', 'ä¸­å›½å¥³ç¯®', 'wcba', 'æ˜“å»ºè”', 
            'éƒ­è‰¾ä¼¦', 'å‘¨ç¦', 'ç‹å“²æ—', 'èµµç»§ä¼Ÿ', 'å¹¿ä¸œå®è¿œ', 'è¾½å®', 
            'åŒ—äº¬é¦–é’¢', 'æ–°ç–†', 'å¹¿å¦', 'ä¸Šæµ·', 'æµ™æ±Ÿ', 'æ·±åœ³', 'å±±ä¸œ',
            'cbaå¸¸è§„èµ›', 'cbaå­£åèµ›', 'æ¨ç‚è', 'å‡†ç»æ€', 'å¥³ç¯®'
        ]
        
        # è®¡ç®—æ¯ä¸ªç±»åˆ«çš„åŒ¹é…åº¦ï¼ˆæ ‡é¢˜ä¸­çš„å…³é”®è¯æƒé‡æ›´é«˜ï¼‰
        title_lower = title.lower()
        content_lower = (content or "").lower()
        
        def calculate_score(keywords, text, title_text):
            score = 0
            for keyword in keywords:
                # æ ‡é¢˜ä¸­çš„å…³é”®è¯æƒé‡ä¸º2ï¼Œå†…å®¹ä¸­çš„æƒé‡ä¸º1
                if keyword in title_text:
                    score += 2
                if keyword in text:
                    score += 1
            return score
        
        esports_score = calculate_score(esports_keywords, content_lower, title_lower)
        soccer_score = calculate_score(soccer_keywords, content_lower, title_lower)
        nba_score = calculate_score(nba_keywords, content_lower, title_lower)
        cba_score = calculate_score(cba_keywords, content_lower, title_lower)
        
        # è¿”å›å¾—åˆ†æœ€é«˜çš„ç±»åˆ«
        scores = {
            'ç”µç«': esports_score,
            'è¶³çƒ': soccer_score,
            'NBA': nba_score,
            'CBA': cba_score
        }
        
        max_score = max(scores.values())
        if max_score >= 2:  # è‡³å°‘éœ€è¦2åˆ†ï¼ˆæ ‡é¢˜ä¸­æœ‰ä¸€ä¸ªå…³é”®è¯ï¼‰æ‰è®¤ä¸ºåŒ¹é…
            # è¿”å›å¾—åˆ†æœ€é«˜çš„ç±»åˆ«
            for category, score in scores.items():
                if score == max_score:
                    return category
        
        # å¦‚æœæ²¡æœ‰åŒ¹é…ï¼Œè¿”å›é»˜è®¤ç±»åˆ«
        return 'ä½“è‚²'
    
    def get_hot_topics(self, category: str = "nba", limit: int = 10) -> List[Dict]:
        """
        è·å–è™æ‰‘çƒ­é—¨è¯é¢˜/çƒè¿·çƒ­è®®
        
        Args:
            category: ç±»åˆ«
            limit: æ•°é‡é™åˆ¶
        
        Returns:
            çƒ­é—¨è¯é¢˜åˆ—è¡¨
        """
        try:
            # å°è¯•ä½¿ç”¨APIè·å–çƒ­é—¨è¯é¢˜
            api_url = f"{self.api_base_url}/bbs/hot"
            params = {
                'category': category,
                'limit': limit
            }
            
            response = requests.get(api_url, headers=self.headers, params=params, timeout=15)
            
            if response.status_code == 200:
                try:
                    data = response.json()
                    topics = []
                    
                    if isinstance(data, dict):
                        items = data.get('data') or data.get('list') or data.get('topics', [])
                    elif isinstance(data, list):
                        items = data
                    else:
                        items = []
                    
                    for item in items[:limit]:
                        if isinstance(item, dict):
                            topic = {
                                "title": item.get('title') or item.get('subject', ''),
                                "content": item.get('content') or item.get('summary', ''),
                                "source": "è™æ‰‘ç¤¾åŒº",
                                "url": item.get('url') or item.get('link', ''),
                                "category": self._map_category(category),
                                "publish_time": self._parse_api_time(item.get('time') or item.get('publish_time')),
                                "reply_count": item.get('reply_count', 0),
                                "view_count": item.get('view_count', 0),
                                "metadata": {
                                    "source_site": "è™æ‰‘",
                                    "source_type": "çƒ­é—¨è¯é¢˜",
                                    "category_code": category
                                }
                            }
                            if topic['title']:
                                topics.append(topic)
                    
                    if topics:
                        print(f"âœ“ ä»è™æ‰‘APIè·å– {len(topics)} æ¡çƒ­é—¨è¯é¢˜")
                        return topics
                except:
                    pass
            
            # å¦‚æœAPIå¤±è´¥ï¼Œè¿”å›ç©ºåˆ—è¡¨
            return []
        except Exception as e:
            print(f"âš ï¸ è·å–çƒ­é—¨è¯é¢˜å¤±è´¥: {str(e)}")
            return []
    
    def get_news_detail(self, url: str) -> Optional[Dict]:
        """è·å–æ–°é—»è¯¦æƒ…"""
        try:
            response = requests.get(url, headers=self.headers, timeout=10)
            response.encoding = 'utf-8'
            
            if response.status_code != 200:
                return None
            
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # æå–æ ‡é¢˜
            title_elem = soup.find(['h1', 'h2'], class_=re.compile(r'title|headline'))
            title = title_elem.get_text(strip=True) if title_elem else ""
            
            # æå–æ­£æ–‡å†…å®¹
            content_selectors = [
                'div.article-content',
                'div.content',
                'div.post-content',
                'div[class*="content"]',
                'article'
            ]
            
            content = ""
            for selector in content_selectors:
                content_elem = soup.select_one(selector)
                if content_elem:
                    # ç§»é™¤è„šæœ¬å’Œæ ·å¼
                    for script in content_elem(["script", "style"]):
                        script.decompose()
                    content = content_elem.get_text(separator='\n', strip=True)
                    if content:
                        break
            
            return {
                "title": title,
                "content": content,
                "url": url
            }
        except Exception as e:
            print(f"è·å–æ–°é—»è¯¦æƒ…å¤±è´¥: {str(e)}")
            return None

def scrape_hupu_news(category: str = "nba", limit: int = 5, use_api: bool = True) -> List[Dict]:
    """
    é‡‡é›†è™æ‰‘æ–°é—»çš„ä¾¿æ·å‡½æ•°ï¼ˆä¼˜å…ˆä½¿ç”¨APIï¼Œå¤±è´¥åˆ™ä½¿ç”¨ç½‘é¡µçˆ¬å–ï¼‰
    
    Args:
        category: æ–°é—»ç±»åˆ«
        limit: è·å–æ•°é‡
        use_api: æ˜¯å¦ä¼˜å…ˆä½¿ç”¨APIæ¥å£
    
    Returns:
        æ–°é—»åˆ—è¡¨
    """
    scraper = HupuScraper()
    
    # ä¼˜å…ˆä½¿ç”¨APIè·å–æŒ‡å®šç±»åˆ«
    if use_api:
        api_news = scraper.get_news_from_api(category, page=1, limit=limit)
        if api_news and len(api_news) >= limit * 0.6:  # å¦‚æœAPIè·å–åˆ°60%ä»¥ä¸Šçš„æ•°æ®ï¼Œç›´æ¥è¿”å›
            return api_news[:limit]
    
    # å¦‚æœAPIå¤±è´¥æˆ–æ•°æ®ä¸è¶³ï¼Œä½¿ç”¨ç½‘é¡µçˆ¬å–
    # å°è¯•å¤šä¸ªç±»åˆ«ä»¥ç¡®ä¿è·å–è¶³å¤Ÿçš„æ–°é—»
    categories = [category, "nba", "soccer", "cba", "news"]
    all_news = []
    
    for cat in categories:
        if len(all_news) >= limit:
            break
        
        news = scraper.get_news_list(cat, limit=limit, use_api=False)  # ç½‘é¡µçˆ¬å–ä¸ä½¿ç”¨API
        # å»é‡
        seen_titles = {item['title'] for item in all_news}
        for item in news:
            if item['title'] not in seen_titles:
                all_news.append(item)
                seen_titles.add(item['title'])
        
        time.sleep(0.5)  # é¿å…è¯·æ±‚è¿‡å¿«
    
    return all_news[:limit]
